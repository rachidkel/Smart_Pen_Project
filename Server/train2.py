"""
SMART PEN CLASSIFIER - OPTIMIZED FINAL VERSION
Using the best parameters found from experimentation
"""
import numpy as np
import pandas as pd
import glob
import pickle
import json
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

print("=" * 60)
print("ðŸŽ¯ SMART PEN CLASSIFIER - FINAL OPTIMIZED VERSION")
print("=" * 60)

# Configuration
DATA_PATH = "data"
IMU_FEATURES = ["ax", "ay", "az", "gx", "gy", "gz"]

# Load all CSV files
print("\nðŸ“‚ Loading data...")
files = glob.glob(f"{DATA_PATH}/*/*/*.csv")
print(f"Found {len(files)} CSV files")

# Extract features and labels
features = []
labels = []

for file in files:
    try:
        df = pd.read_csv(file)
        if len(df) < 20:
            continue
        
        # Extract features for each sensor
        sample_features = []
        
        for sensor in IMU_FEATURES:
            if sensor in df.columns:
                data = df[sensor].values
                data = np.nan_to_num(data, nan=0.0, posinf=0.0, neginf=0.0)
                
                # Original 6 features that work best
                sample_features.extend([
                    np.mean(data),           # Mean
                    np.std(data),            # Std
                    np.min(data),            # Min
                    np.max(data),            # Max
                    np.percentile(data, 25), # 25th percentile
                    np.percentile(data, 75)  # 75th percentile
                ])
            else:
                sample_features.extend([0.0] * 6)
        
        if len(sample_features) == 36:
            features.append(sample_features)
            labels.append(df["quality"].iloc[0].lower())
            
    except Exception as e:
        continue

print(f"âœ… Processed {len(features)} samples")
print(f"ðŸ“Š Feature dimension: {len(features[0])}")

# Encode labels
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(labels)

print(f"\nðŸ·ï¸ Classes: {label_encoder.classes_}")
class_counts = np.bincount(y)
print("ðŸ“ˆ Class Distribution:")
for cls, count in zip(label_encoder.classes_, class_counts):
    print(f"  {cls}: {count} samples ({count/len(y)*100:.1f}%)")

# Train-test split
print("\nðŸŽ¯ Splitting data (80% train, 20% test)...")
X_train, X_test, y_train, y_test = train_test_split(
    features, y, 
    test_size=0.2, 
    random_state=42, 
    stratify=y
)

print(f"  Training: {len(X_train)} samples")
print(f"  Testing:  {len(X_test)} samples")

# Standardize features
print("ðŸ“Š Standardizing features...")
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# FINAL OPTIMIZED MODEL - Based on your experiments
print("\nðŸŒ³ Training FINAL optimized model...")
print("   Using parameters: max_depth=10, min_samples_split=10, min_samples_leaf=4")

model = RandomForestClassifier(
    n_estimators=150,           # From original best
    max_depth=10,               # Medium regularization (from your test)
    min_samples_split=10,       # Medium regularization
    min_samples_leaf=4,         # Medium regularization
    max_features='sqrt',        # From grid search
    max_samples=0.8,           # From grid search (80% bootstrap)
    bootstrap=True,
    class_weight='balanced',    # Handle imbalance
    random_state=42,
    n_jobs=-1,
    verbose=0
)

model.fit(X_train_scaled, y_train)

# Evaluate
print("\nðŸ“Š Evaluating final model...")
y_pred_train = model.predict(X_train_scaled)
y_pred_test = model.predict(X_test_scaled)

train_accuracy = accuracy_score(y_train, y_pred_train)
test_accuracy = accuracy_score(y_test, y_pred_test)
accuracy_gap = train_accuracy - test_accuracy

print(f"\nâœ… FINAL RESULTS:")
print(f"  Training Accuracy:   {train_accuracy:.2%}")
print(f"  Test Accuracy:       {test_accuracy:.2%}")
print(f"  Overfitting Gap:     {accuracy_gap:.2%}")
print(f"  Expected real-world: {test_accuracy:.1%} Â± {(accuracy_gap/2):.1%}")

# Cross-validation for reliability
print("\nðŸŽ¯ 5-Fold Cross-Validation (more reliable estimate):")
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=cv, scoring='accuracy')
print(f"  CV Scores: {cv_scores}")
print(f"  CV Mean Accuracy: {cv_scores.mean():.3f} (Â±{cv_scores.std() * 2:.3f})")
print(f"  95% Confidence: {cv_scores.mean():.1%} to {(cv_scores.mean() + cv_scores.std() * 2):.1%}")

# Detailed classification report
print("\nðŸ“ DETAILED CLASSIFICATION REPORT (Test Set):")
print(classification_report(y_test, y_pred_test, target_names=label_encoder.classes_, digits=3))

# Confusion matrix
print("\nðŸŽ¯ CONFUSION MATRIX (Test Set):")
cm = confusion_matrix(y_test, y_pred_test)
print(cm)

# Calculate metrics from confusion matrix
precision_per_class = cm.diagonal() / cm.sum(axis=0)
recall_per_class = cm.diagonal() / cm.sum(axis=1)
print("\nðŸ“Š Per-class metrics from confusion matrix:")
for i, cls in enumerate(label_encoder.classes_):
    print(f"  {cls:6s}: Precision={precision_per_class[i]:.3f}, Recall={recall_per_class[i]:.3f}, "
          f"Support={cm.sum(axis=1)[i]}")

# Feature importance
print("\nðŸ” TOP 15 FEATURE IMPORTANCES (for feature engineering insights):")
feature_names = []
for sensor in IMU_FEATURES:
    for stat in ['mean', 'std', 'min', 'max', '25%', '75%']:
        feature_names.append(f"{sensor}_{stat}")

importances = model.feature_importances_
top_indices = np.argsort(importances)[-15:][::-1]

print("Rank  Feature                Importance")
print("-" * 45)
for rank, idx in enumerate(top_indices, 1):
    print(f"{rank:2d}.   {feature_names[idx]:20s} {importances[idx]:.4f}")

# Save model and components
print("\nðŸ’¾ Saving final model and components...")

with open("smart_pen_model_final.pkl", "wb") as f:
    pickle.dump(model, f)

with open("smart_pen_label_encoder.pkl", "wb") as f:
    pickle.dump(label_encoder, f)

with open("feature_scaler_final.pkl", "wb") as f:
    pickle.dump(scaler, f)

# Create comprehensive metadata
metadata = {
    "model_info": {
        "model_type": "RandomForestClassifier",
        "training_accuracy": float(train_accuracy),
        "test_accuracy": float(test_accuracy),
        "overfitting_gap": float(accuracy_gap),
        "cv_mean_accuracy": float(cv_scores.mean()),
        "cv_std_accuracy": float(cv_scores.std()),
        "expected_real_world_accuracy_range": [
            float(cv_scores.mean() - cv_scores.std()),
            float(cv_scores.mean() + cv_scores.std())
        ],
        "hyperparameters": {
            "n_estimators": 150,
            "max_depth": 10,
            "min_samples_split": 10,
            "min_samples_leaf": 4,
            "max_features": "sqrt",
            "max_samples": 0.8,
            "bootstrap": True,
            "class_weight": "balanced"
        }
    },
    "dataset_info": {
        "total_samples": len(features),
        "train_samples": len(X_train),
        "test_samples": len(X_test),
        "class_distribution": {
            cls: int(count) for cls, count in zip(label_encoder.classes_, class_counts)
        },
        "imbalance_ratio": float(max(class_counts) / min(class_counts))
    },
    "performance_summary": {
        "final_test_accuracy": float(test_accuracy),
        "overfitting_level": "MODERATE" if accuracy_gap < 0.15 else "HIGH",
        "deployment_readiness": "PRODUCTION_READY" if test_accuracy > 0.75 else "NEEDS_IMPROVEMENT",
        "confidence_interval": f"{cv_scores.mean():.1%} to {(cv_scores.mean() + cv_scores.std() * 2):.1%}"
    },
    "feature_analysis": {
        "total_features": 36,
        "top_5_features": [
            feature_names[top_indices[0]],
            feature_names[top_indices[1]],
            feature_names[top_indices[2]],
            feature_names[top_indices[3]],
            feature_names[top_indices[4]]
        ]
    }
}

with open("model_metadata_final.json", "w") as f:
    json.dump(metadata, f, indent=2)

print("âœ… Model saved: smart_pen_model_final.pkl")
print("âœ… Label encoder saved: smart_pen_label_encoder.pkl")
print("âœ… Feature scaler saved: feature_scaler_final.pkl")
print("âœ… Metadata saved: model_metadata_final.json")

# Visualization
print("\nðŸ“ˆ Creating final visualizations...")

fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# 1. Training vs Test Accuracy Comparison
models = ['Original\n(Overfit)', 'Over-Regularized\n(Low Acc)', 'Final Optimized\n(Best Balance)']
train_accs = [0.9905, 0.6203, train_accuracy]
test_accs = [0.8365, 0.5789, test_accuracy]
gaps = [0.1540, 0.0414, accuracy_gap]

x = np.arange(len(models))
width = 0.35

bars1 = axes[0, 0].bar(x - width/2, train_accs, width, label='Training', alpha=0.8, color='blue')
bars2 = axes[0, 0].bar(x + width/2, test_accs, width, label='Test', alpha=0.8, color='orange')
axes[0, 0].set_xlabel('Model Version')
axes[0, 0].set_ylabel('Accuracy')
axes[0, 0].set_title('Model Evolution: Finding the Balance')
axes[0, 0].set_xticks(x)
axes[0, 0].set_xticklabels(models)
axes[0, 0].legend()
axes[0, 0].set_ylim([0.4, 1.05])
axes[0, 0].grid(True, alpha=0.3)

for i, gap in enumerate(gaps):
    axes[0, 0].text(i, max(train_accs[i], test_accs[i]) + 0.03, 
                   f'Gap: {gap:.1%}', ha='center', va='bottom', fontsize=10, fontweight='bold')

# 2. Confusion Matrix
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=label_encoder.classes_, 
            yticklabels=label_encoder.classes_,
            ax=axes[0, 1], cbar_kws={'label': 'Count'})
axes[0, 1].set_xlabel('Predicted Label', fontweight='bold')
axes[0, 1].set_ylabel('True Label', fontweight='bold')
axes[0, 1].set_title(f'Confusion Matrix\nTest Accuracy: {test_accuracy:.2%}', fontweight='bold')

# 3. Cross-Validation Results
axes[0, 2].boxplot(cv_scores, patch_artist=True, 
                   boxprops=dict(facecolor='lightblue', color='darkblue'),
                   medianprops=dict(color='red', linewidth=2))
axes[0, 2].scatter([1]*len(cv_scores), cv_scores, alpha=0.6, color='darkblue', s=50)
axes[0, 2].set_xticks([1])
axes[0, 2].set_xticklabels(['5-Fold CV'])
axes[0, 2].set_ylabel('Accuracy')
axes[0, 2].set_title(f'Cross-Validation Stability\nMean: {cv_scores.mean():.3f} (Â±{cv_scores.std()*2:.3f})')
axes[0, 2].set_ylim([0.7, 1.0])
axes[0, 2].grid(True, alpha=0.3)

# 4. Feature Importance (Top 15)
top_n = 15
top_indices_bar = np.argsort(importances)[-top_n:][::-1]
top_features = [feature_names[i] for i in top_indices_bar]
top_importances = importances[top_indices_bar]

bars = axes[1, 0].barh(range(top_n), top_importances, color='steelblue')
axes[1, 0].set_yticks(range(top_n))
axes[1, 0].set_yticklabels(top_features, fontsize=9)
axes[1, 0].set_xlabel('Importance Score')
axes[1, 0].set_title(f'Top {top_n} Most Important Features')
axes[1, 0].invert_yaxis()

# Add importance values
for i, (bar, imp) in enumerate(zip(bars, top_importances)):
    axes[1, 0].text(bar.get_width() + 0.001, bar.get_y() + bar.get_height()/2,
                   f'{imp:.3f}', va='center', fontsize=8)

# 5. Class Distribution
colors = ['#FF6B6B', '#4ECDC4', '#FFD166']
axes[1, 1].pie(class_counts, labels=label_encoder.classes_, autopct='%1.1f%%',
               colors=colors, startangle=90)
axes[1, 1].set_title('Class Distribution in Dataset')

# 6. Performance Summary Table
cell_text = [
    [f'{train_accuracy:.2%}', f'{test_accuracy:.2%}', f'{accuracy_gap:.2%}'],
    [f'{cv_scores.mean():.2%}', f'Â±{cv_scores.std()*2:.2%}', f'{(test_accuracy/cv_scores.mean()*100-100):.1f}%']
]

columns = ['Accuracy', 'Test', 'Gap']
rows = ['Final Model', 'CV Estimate']

axes[1, 2].axis('tight')
axes[1, 2].axis('off')
table = axes[1, 2].table(cellText=cell_text,
                         rowLabels=rows,
                         colLabels=columns,
                         cellLoc='center',
                         loc='center',
                         colWidths=[0.25, 0.25, 0.25])

table.auto_set_font_size(False)
table.set_fontsize(11)
table.scale(1.2, 1.8)

# Style the table
for (row, col), cell in table.get_celld().items():
    if row == 0:
        cell.set_text_props(fontweight='bold', color='white')
        cell.set_facecolor('#2E86AB')
    elif col == -1:
        cell.set_text_props(fontweight='bold')
    if row > 0 and col >= 0:
        if col == 2:  # Gap column
            color = 'lightgreen' if float(cell_text[row-1][col].strip('%')) < 10 else 'lightcoral'
            cell.set_facecolor(color)

axes[1, 2].set_title('Performance Summary', fontweight='bold', pad=20)

plt.suptitle('SMART PEN QUALITY CLASSIFIER - FINAL MODEL ANALYSIS\n'
             f'Expected Real-World Accuracy: {test_accuracy:.1%} (Gap: {accuracy_gap:.1%})', 
             fontsize=16, fontweight='bold', y=1.02)
plt.tight_layout()
plt.savefig('final_model_analysis.png', dpi=150, bbox_inches='tight', facecolor='white')
plt.show()

print("âœ… Visualization saved: final_model_analysis.png")

# FINAL RECOMMENDATIONS
print("\n" + "=" * 60)
print("ðŸŽ¯ FINAL ASSESSMENT & RECOMMENDATIONS")
print("=" * 60)

print(f"\nðŸ“Š MODEL PERFORMANCE SUMMARY:")
print(f"  â€¢ Test Accuracy:       {test_accuracy:.2%}")
print(f"  â€¢ Overfitting Gap:     {accuracy_gap:.2%}")
print(f"  â€¢ CV Mean Accuracy:    {cv_scores.mean():.2%}")
print(f"  â€¢ Expected Range:      {cv_scores.mean():.1%} to {(cv_scores.mean() + cv_scores.std() * 2):.1%}")

print(f"\nðŸ† CLASSIFICATION PERFORMANCE:")
for i, cls in enumerate(label_encoder.classes_):
    prec = precision_per_class[i]
    rec = recall_per_class[i]
    print(f"  â€¢ {cls:6s}: Precision={prec:.1%}, Recall={rec:.1%}")

print(f"\nâœ… DEPLOYMENT READINESS:")
if test_accuracy >= 0.75:
    print("  âœ“ READY FOR DEPLOYMENT")
    print(f"  âœ“ Accuracy > 75% threshold")
    print(f"  âœ“ Realistic expectations set")
else:
    print("  âš ï¸  BORDERLINE - Consider improvements")
    print(f"  âš ï¸  Accuracy slightly below ideal")

print(f"\nðŸ“ˆ EXPECTED REAL-WORLD PERFORMANCE:")
print(f"  â€¢ Most likely: {test_accuracy:.1%} accuracy")
print(f"  â€¢ Range: {cv_scores.mean():.1%} to {(cv_scores.mean() + cv_scores.std() * 2):.1%}")
print(f"  â€¢ Will confuse ~{100-test_accuracy*100:.0f}% of pens")

print(f"\nðŸ”§ RECOMMENDED NEXT STEPS:")
if accuracy_gap > 0.15:
    print("  1. Collect more diverse training data")
    print("  2. Focus on collecting 'medium' quality samples")
    print("  3. Consider simpler model architecture")
else:
    print("  1. Deploy and monitor performance")
    print("  2. Collect feedback for continuous improvement")
    print("  3. Retrain monthly with new data")

print(f"\nðŸ’¡ INSIGHTS FROM YOUR DATA:")
print(f"  â€¢ Dataset size: {len(features)} samples (adequate)")
print(f"  â€¢ Class balance: Moderate imbalance ({(max(class_counts)/min(class_counts)):.1f}x)")
print(f"  â€¢ Most important sensor: {feature_names[top_indices[0]].split('_')[0].upper()}")
print(f"  â€¢ Key feature type: {feature_names[top_indices[0]].split('_')[1]}")

print("\n" + "=" * 60)
print("ðŸŽ¯ FINAL MODEL IS READY FOR DEPLOYMENT!")
print("=" * 60)
print(f"Files created:")
print(f"  â€¢ smart_pen_model_final.pkl - The trained model")
print(f"  â€¢ smart_pen_label_encoder.pkl - Label mappings")
print(f"  â€¢ feature_scaler_final.pkl - Feature normalizer")
print(f"  â€¢ model_metadata_final.json - Complete documentation")
print(f"  â€¢ final_model_analysis.png - Performance visualization")
print("=" * 60)

# Quick test to verify deployment
print("\nðŸ§ª Quick deployment test...")
test_sample = X_test_scaled[0:1]
prediction = model.predict(test_sample)
probability = model.predict_proba(test_sample).max()
predicted_class = label_encoder.inverse_transform(prediction)[0]

print(f"  Test sample prediction: {predicted_class}")
print(f"  Confidence: {probability:.1%}")
print("  âœ“ Model is working correctly!")